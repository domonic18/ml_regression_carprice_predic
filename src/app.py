#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ç³»ç»Ÿçš„ Streamlit å‰ç«¯åº”ç”¨ç¨‹åºã€‚
æœ¬æ¨¡å—å®ç°äº†streamlitçš„å‰ç«¯é¡µé¢ï¼ŒåŠŸèƒ½åŒ…æ‹¬ï¼š
1. æ•°æ®åˆ†æ
2. æ•°æ®æ¸…ç†
3. æ¨¡å‹è®­ç»ƒ
4. æ•ˆæœéªŒè¯

ä½œè€…: è¯¸è‘›ä¸œæ˜
æ—¥æœŸ: 2025-01-04
ç‰ˆæœ¬: 1.0.0
"""

import streamlit as st
import pandas as pd
import torch
import os
from ydata_profiling import ProfileReport
from data_processing.data_processor import DataProcessor
from models.car_price_model import CarPriceModel
from train import train_car_price_model
import numpy as np
import logging

def update_progress(epoch, train_loss, test_loss):
    """
    æ›´æ–°è®­ç»ƒè¿›åº¦çš„å›è°ƒå‡½æ•°
    
    Args:
        epoch (int): å½“å‰è®­ç»ƒè½®æ¬¡
        train_loss (float): è®­ç»ƒæŸå¤±
        test_loss (float): æµ‹è¯•æŸå¤±
    """
    if not hasattr(update_progress, 'train_losses'):
        update_progress.train_losses = []
        update_progress.test_losses = []
    
    # æ›´æ–°è¿›åº¦æ¡
    progress = (epoch + 1) / st.session_state.get('total_epochs', 100)
    st.session_state.progress_bar.progress(progress)
    
    # æ›´æ–°çŠ¶æ€æ–‡æœ¬
    st.session_state.status_text.text(
        f'è®­ç»ƒè½®æ¬¡ {epoch+1}/{st.session_state.get("total_epochs", 100)} - '
        f'è®­ç»ƒæŸå¤±: {train_loss:.5f}, æµ‹è¯•æŸå¤±: {test_loss:.5f}'
    )
    
    # æ›´æ–°æŸå¤±å›¾è¡¨
    update_progress.train_losses.append(train_loss)
    update_progress.test_losses.append(test_loss)
    loss_df = pd.DataFrame({
        "è®­ç»ƒæŸå¤±": update_progress.train_losses,
        "æµ‹è¯•æŸå¤±": update_progress.test_losses
    })
    st.session_state.loss_chart.line_chart(loss_df)

def train_model(epochs, learning_rate):
    """
    åœ¨Streamlitç•Œé¢ä¸­è®­ç»ƒæ¨¡å‹å¹¶æ˜¾ç¤ºè¿›åº¦
    
    Args:
        epochs (int): è®­ç»ƒè½®æ•°
        learning_rate (float): å­¦ä¹ ç‡
    """
    # åˆ›å»ºè¿›åº¦æ¡å’ŒçŠ¶æ€æ˜¾ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    loss_chart = st.empty()
    
    # åˆ›å»ºç”¨äºå®æ—¶æ˜¾ç¤ºæŸå¤±çš„åˆ—è¡¨
    train_losses = []
    test_losses = []
    
    try:
        # è°ƒç”¨è®­ç»ƒå‡½æ•°
        model, train_losses, test_losses, processor = train_car_price_model(
            st.session_state.uploaded_file.name,
            epochs=epochs,
            learning_rate=learning_rate,
            progress_callback=update_progress
        )
        
        # ä¿å­˜æ¨¡å‹
        model_save_path = "trained_model.pth"
        torch.save(model.state_dict(), model_save_path)
        
        # ä¿å­˜ç¼©æ”¾å‚æ•° - ç°åœ¨processoræ˜¯è®­ç»ƒå‡½æ•°è¿”å›çš„
        if processor is not None:  # æ·»åŠ æ£€æŸ¥
            scaling_params_path = "scaling_params.npz"
            np.savez(
                scaling_params_path,
                mean_X=processor.mean_X,
                std_X=processor.std_X,
                mean_y=processor.mean_y,
                std_y=processor.std_y
            )
        else:
            st.warning("æœªèƒ½è·å–æ•°æ®å¤„ç†å™¨å®ä¾‹ï¼Œç¼©æ”¾å‚æ•°æœªä¿å­˜")
        
        return model, train_losses, test_losses
        
    except Exception as e:
        st.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return None, None, None

def load_model(model_path, input_features):
    """
    åŠ è½½æ¨¡å‹å¹¶å‡†å¤‡é¢„æµ‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡
        input_features: è¾“å…¥ç‰¹å¾æ•°é‡
        
    Returns:
        model: åŠ è½½å¥½çš„æ¨¡å‹
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = CarPriceModel(in_features=input_features, out_features=1)
    
    try:
        # å¦‚æœè¾“å…¥æ˜¯æ–‡ä»¶å¯¹è±¡ï¼Œéœ€è¦å…ˆè¯»å–å†…å®¹
        if hasattr(model_path, 'read'):
            model_data = model_path.read()
            import io
            buffer = io.BytesIO(model_data)
            # å°è¯•åŠ è½½æ¨¡å‹
            loaded = torch.load(buffer, map_location=device)
        else:
            loaded = torch.load(model_path, map_location=device)
        
        # åˆ¤æ–­åŠ è½½çš„æ˜¯æ•´ä¸ªæ¨¡å‹è¿˜æ˜¯çŠ¶æ€å­—å…¸
        if isinstance(loaded, CarPriceModel):
            # å¦‚æœåŠ è½½çš„æ˜¯æ•´ä¸ªæ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨
            model = loaded
        else:
            # å¦‚æœåŠ è½½çš„æ˜¯çŠ¶æ€å­—å…¸ï¼ŒåŠ è½½å‚æ•°
            model.load_state_dict(loaded)
            
        model.to(device)
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        return model
        
    except Exception as e:
        raise Exception(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")

def predict_price(model, features):
    """
    ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
    
    Args:
        model: åŠ è½½å¥½çš„æ¨¡å‹
        features: é¢„å¤„ç†åçš„ç‰¹å¾æ•°æ®
        
    Returns:
        float: é¢„æµ‹ä»·æ ¼
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯æ­£ç¡®çš„å½¢çŠ¶å’Œç±»å‹
        if isinstance(features, pd.Series):
            features = features.values
            
        if len(features.shape) == 1:
            features = features.reshape(1, -1)
            
        # è½¬æ¢ä¸ºtensorå¹¶ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        features = torch.FloatTensor(features).to(device)
        
        # è¿›è¡Œé¢„æµ‹
        with torch.no_grad():
            prediction = model(features)
            return prediction.item()
            
    except Exception as e:
        raise Exception(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

def show_file_uploader():
    """æ˜¾ç¤ºæ–‡ä»¶ä¸Šä¼ ç»„ä»¶"""
    uploaded_file = st.sidebar.file_uploader("é€‰æ‹©CSVæ–‡ä»¶", type="csv", key="shared_uploader")
    if uploaded_file and uploaded_file != st.session_state.get('uploaded_file'):
        st.session_state.uploaded_file = uploaded_file
        # è¯»å–CSVæ—¶æŒ‡å®šæ•°æ®ç±»å‹
        try:
            df = pd.read_csv(uploaded_file, delimiter='\s+')
            # å°è¯•å°†å¯èƒ½æ˜¯æ•°å€¼çš„åˆ—è½¬æ¢ä¸ºfloatç±»å‹
            for col in df.columns:
                try:
                    if df[col].dtype == 'object':  # å¦‚æœåˆ—æ˜¯å¯¹è±¡ç±»å‹
                        df[col] = pd.to_numeric(df[col], errors='coerce')  # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                except Exception as e:
                    logging.warning(f"åˆ— {col} è½¬æ¢ç±»å‹æ—¶å‡ºé”™: {str(e)}")
            
            st.session_state.df = df
        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return None
    return uploaded_file

def data_analysis():
    """æ•°æ®åˆ†æåŠŸèƒ½"""
    st.sidebar.header("æ•°æ®ä¸Šä¼ ")
    uploaded_file = show_file_uploader()
    
    if uploaded_file:
        st.dataframe(st.session_state.df)

        if st.sidebar.button("åˆ†ææ•°æ®"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šï¼Œè¯·ç¨å€™..."):
                profile = ProfileReport(st.session_state.df, title="æ•°æ®åˆ†ææŠ¥å‘Š", minimal=True)
                report_html = profile.to_html()
                st.subheader("Pandas Profiling Report")
                st.components.v1.html(report_html, width=1000, height=550, scrolling=True)

def data_cleaning():
    """æ•°æ®æ¸…æ´—åŠŸèƒ½"""
    st.sidebar.header("æ•°æ®ä¸Šä¼ ")
    uploaded_file = show_file_uploader()
    
    if uploaded_file:
        if st.sidebar.button("å¼€å§‹æ•°æ®æ¸…æ´—", type="primary"):
            processor = DataProcessor()
            with st.spinner("æ­£åœ¨æ¸…æ´—æ•°æ®..."):
                X, y = processor.load_and_analyze_data(st.session_state.df)
                cleaned_df = pd.DataFrame(X, columns=X.columns)
                cleaned_df['price'] = y  # æ·»åŠ priceåˆ—
                st.session_state.cleaned_data = (X, y)
                st.session_state.cleaned_df = cleaned_df
            
            st.success("æ•°æ®æ¸…æ´—å®Œæˆï¼")
            
        # åœ¨ä¾§è¾¹æ æ·»åŠ ä¿å­˜ç›¸å…³æ§ä»¶
        if 'cleaned_df' in st.session_state:
            st.sidebar.markdown("---")  # æ·»åŠ åˆ†éš”çº¿
            st.sidebar.subheader("ä¿å­˜æ¸…æ´—åçš„æ•°æ®")
            
            # æ·»åŠ ç›®å½•é€‰æ‹©è¾“å…¥æ¡†åˆ°ä¾§è¾¹æ ,é»˜è®¤ä¿å­˜åˆ°ä¸Šçº§çš„dataç›®å½•
            current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            default_save_dir = os.path.join(current_dir, "data")
            
            save_dir = st.sidebar.text_input(
                "ä¿å­˜ç›®å½•",
                value=default_save_dir,
                help="è¾“å…¥è¦ä¿å­˜æ´—åæ•°æ®çš„ç›®å½•è·¯å¾„",
                key="save_dir_input"
            )
            
            if st.sidebar.button("ä¿å­˜æ•°æ®"):
                original_filename = os.path.splitext(st.session_state.uploaded_file.name)[0]
                cleaned_filename = f"{original_filename}_cleaned.csv"
                
                try:
                    save_path = os.path.join(save_dir, cleaned_filename)
                    os.makedirs(save_dir, exist_ok=True)
                    st.session_state.cleaned_df.to_csv(save_path, index=False)
                    st.sidebar.success(f"æ•°æ®å·²ä¿å­˜è‡³: {save_path}")
                except Exception as e:
                    st.sidebar.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            
            # åœ¨é¡µé¢æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            if 'cleaned_df' in st.session_state:
                st.subheader("æ¸…æ´—åçš„æ•°æ®é¢„è§ˆ")
                st.dataframe(st.session_state.cleaned_df.head(10))
        else:
            st.warning("è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶")

def model_training():
    """æ¨¡å‹è®­ç»ƒåŠŸèƒ½"""
    st.sidebar.header("è®­ç»ƒå‚æ•°è®¾ç½®")
    training_file = st.sidebar.file_uploader(
        "é€‰æ‹©è®­ç»ƒæ•°æ®æ–‡ä»¶", 
        type="csv", 
        key="training_file_uploader"
    )
    if training_file:
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        st.subheader("è®­ç»ƒæ•°æ®ä¿¡æ¯")
        st.write(f"æ–‡ä»¶å: {training_file.name}")
        
        # è¯»å–å¹¶æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        df = pd.read_csv(training_file)
        st.write(f"æ•°æ®æ€»è¡Œæ•°: {len(df)}")
        st.write("æ•°æ®é¢„è§ˆ (å‰10è¡Œ):")
        st.dataframe(df.head(10))
        
        # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        st.write("æ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
        st.dataframe(df.describe())
        
        # è®­ç»ƒå‚æ•°è®¾ç½®
        epochs = st.sidebar.number_input("è®­ç»ƒè½®æ•°", value=100, min_value=1)
        learning_rate = st.sidebar.number_input("å­¦ä¹ ç‡", value=1e-4, format="%.4f")
        
        if st.sidebar.button("å¼€å§‹è®­ç»ƒ"):
            # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºç»„ä»¶
            st.session_state.progress_bar = st.progress(0)
            st.session_state.status_text = st.empty()
            st.session_state.loss_chart = st.empty()
            st.session_state.total_epochs = epochs
            
            with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹..."):
                model, train_losses, test_losses, processor = train_car_price_model(
                    df,
                    epochs=epochs,
                    learning_rate=learning_rate,
                    progress_callback=update_progress
                )
                # ä¿å­˜æ¨¡å‹å’Œç¼©æ”¾å‚æ•°
                try:
                    # ä¿å­˜æ¨¡å‹
                    model_save_path = "model.pth"
                    torch.save(model.state_dict(), model_save_path)
                    
                    # ä¿å­˜ç¼©æ”¾å‚æ•°
                    if processor is not None:
                        scaling_params_path = "scaling_params.npz"
                        np.savez(
                            scaling_params_path,
                            mean_X=processor.mean_X,
                            std_X=processor.std_X,
                            mean_y=processor.mean_y,
                            std_y=processor.std_y
                        )
                        st.write(f"mean_X: {processor.mean_X}")
                        st.write(f"std_X: {processor.std_X}")
                        st.write(f"mean_y: {processor.mean_y}")
                        st.write(f"std_y: {processor.std_y}")
                        st.success(f"æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")
                        st.success(f"ç¼©æ”¾å‚æ•°å·²ä¿å­˜è‡³: {scaling_params_path}")
                    else:
                        st.warning("æœªèƒ½è·å–æ•°æ®å¤„ç†å™¨å®ä¾‹ï¼Œç¼©æ”¾å‚æ•°æœªä¿å­˜")
                except Exception as e:
                    st.error(f"ä¿å­˜æ¨¡å‹æˆ–ç¼©æ”¾å‚æ•°æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    
            st.success("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            st.line_chart(pd.DataFrame({
                "è®­ç»ƒæŸå¤±": train_losses,
                "æµ‹è¯•æŸå¤±": test_losses
            }))

def model_validation():
    """æ•ˆæœéªŒè¯åŠŸèƒ½"""
    st.sidebar.header("ğŸ” æ¨¡å‹è½½å…¥")
    
    # è·å–å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰.pthå’Œ.npzæ–‡ä»¶
    current_dir = os.path.dirname(os.path.abspath(__file__))
    model_files = [f for f in os.listdir(current_dir) if f.endswith('.pth')]
    scaling_param_files = [f for f in os.listdir(current_dir) if f.endswith('.npz')]
    
    # åˆ›å»ºä¸‹æ‹‰æ¡†é€‰æ‹©æ¨¡å‹æ–‡ä»¶å’Œç¼©æ”¾å‚æ•°æ–‡ä»¶
    model_file = st.sidebar.selectbox("ğŸ“ é€‰æ‹©æ¨¡å‹æ–‡ä»¶", model_files)
    scaling_params_file = st.sidebar.selectbox("ğŸ“ é€‰æ‹©ç¼©æ”¾å‚æ•°æ–‡ä»¶", scaling_param_files)
    
    # æ·»åŠ éšæœºè¯»å–æ•°æ®çš„æŒ‰é’®
    if st.sidebar.button("ğŸ² éšæœºè¯»å–æ–°æ•°æ®"):
        # æ¸…é™¤å·²ä¿å­˜çš„æ ·æœ¬æ•°æ®ï¼Œè§¦å‘é‡æ–°éšæœºé€‰æ‹©
        st.session_state.pop('sample_data', None)
    
    if model_file and scaling_params_file:
        # æ„å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        test_file = os.path.join(os.path.dirname(current_dir), "data", "used_car_testB_20200421.csv")
        
        if os.path.exists(test_file):
            # åªåœ¨é¦–æ¬¡åŠ è½½æˆ–ç‚¹å‡»éšæœºæŒ‰é’®æ—¶å¤„ç†æ•°æ®
            if 'sample_data' not in st.session_state:
                test_df = pd.read_csv(test_file, delimiter='\s+')
                processor = DataProcessor()
                sample_df = processor.preprocess_sample(test_df)
                # éšæœºé€‰æ‹©ä¸€æ¡é¢„å¤„ç†åçš„æ•°æ®å¹¶å­˜å‚¨åœ¨session stateä¸­
                st.session_state.sample_data = pd.DataFrame(sample_df).sample(n=1).iloc[0]
            
            # ä½¿ç”¨å­˜å‚¨çš„æ ·æœ¬æ•°æ®
            sample_data = st.session_state.sample_data

            # æ˜¾ç¤ºè¾“å…¥æ•°æ®
            st.subheader("ğŸ“ æ ·æœ¬æ•°æ®")
            
            # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
            col1, col2 = st.columns(2)
            
            # ä¸ºæ¯ä¸ªç‰¹å¾åˆ›å»ºç¼–è¾‘æ¡†
            edited_values = {}
            for i, (feature, value) in enumerate(sample_data.items()):
                # åœ¨å·¦åˆ—æˆ–å³åˆ—æ˜¾ç¤ºç¼–è¾‘æ¡†
                with col1 if i % 2 == 0 else col2:
                    edited_values[feature] = st.number_input(
                        f"ğŸ“Š {feature}",
                        value=float(value),
                        format="%.2f",
                        key=f"feature_{feature}"
                    )
            
            # ä½¿ç”¨ç¼–è¾‘åçš„å€¼åˆ›å»ºæ–°çš„æ ·æœ¬æ•°æ®
            edited_sample_data = pd.Series(edited_values)
            
            if st.button("ğŸ¯ å¼€å§‹é¢„æµ‹"):
                try:
                    # åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„ç¼©æ”¾å‚æ•°
                    scaling_params_path = os.path.join(current_dir, scaling_params_file)
                    scaling_params = np.load(scaling_params_path, allow_pickle=True)
                    processor = DataProcessor()
                    processor.mean_y = scaling_params['mean_y']
                    processor.std_y = scaling_params['std_y']
                    processor.mean_X = scaling_params['mean_X']
                    processor.std_X = scaling_params['std_X']
                    
                    if processor.mean_X is None or processor.std_X is None:
                        st.error("âŒ ç¼©æ”¾å‚æ•°æœªæ­£ç¡®åŠ è½½ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å’Œç¼©æ”¾å‚æ•°æ–‡ä»¶ã€‚")
                    else:
                        # ä½¿ç”¨ç¼©æ”¾å‚æ•°è¿›è¡Œæ ‡å‡†åŒ–
                        X = (edited_sample_data - processor.mean_X) / processor.std_X
                        X = X.astype(np.float32)
                        
                        # åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹
                        model_path = os.path.join(current_dir, model_file)
                        model = load_model(model_path, len(X))
                        predicted_price = predict_price(model, X)
                        predicted_price = predicted_price * processor.std_y + processor.mean_y
                        
                        st.success(f"ğŸ’° é¢„æµ‹ä»·æ ¼ï¼šÂ¥{predicted_price:.2f}")
                        
                except Exception as e:
                    st.error(f"âŒ é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        else:
            st.error(f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼š{test_file}")
    else:
        st.warning("âš ï¸ è¯·ä¸Šä¼ æ¨¡å‹æ–‡ä»¶å’Œç¼©æ”¾å‚æ•°æ–‡ä»¶")

def main():
    st.title("ğŸš— äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ç³»ç»Ÿ")
    
    # æ·»åŠ é¡¹ç›®è¯´æ˜
    if not st.session_state.get('uploaded_file'):
        st.markdown("""
        ### ğŸ‘‹ æ¬¢è¿ä½¿ç”¨äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ç³»ç»Ÿï¼
        
        æœ¬ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹å·¥å…·ï¼Œæä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š
        
        - ğŸ“Š **æ•°æ®åˆ†æ**ï¼šå¯¹äºŒæ‰‹è½¦æ•°æ®è¿›è¡Œå¯è§†åŒ–åˆ†æå’Œç»Ÿè®¡
        - ğŸ§¹ **æ•°æ®æ¸…æ´—**ï¼šè‡ªåŠ¨å¤„ç†å’Œæ¸…ç†åŸå§‹æ•°æ®
        - ğŸ”„ **æ¨¡å‹è®­ç»ƒ**ï¼šä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒä»·æ ¼é¢„æµ‹å™¨
        - âœ¨ **æ•ˆæœéªŒè¯**ï¼šéªŒè¯æ¨¡å‹é¢„æµ‹æ•ˆæœå¹¶è¿›è¡Œå®æ—¶é¢„æµ‹
        
        #### ä½¿ç”¨è¯´æ˜
        1. åœ¨å·¦ä¾§è¾¹æ é€‰æ‹©éœ€è¦ä½¿ç”¨çš„åŠŸèƒ½
        2. ä¸Šä¼ æ•°æ®æ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼‰
        3. æ ¹æ®ç•Œé¢æç¤ºè¿›è¡Œæ“ä½œ
        
        #### å…³äºæˆ‘ä»¬
        æœ¬é¡¹ç›®ç”±[17AIæŠ€æœ¯ç¤¾åŒº](http://17aitech.com)å¼€å‘å’Œç»´æŠ¤ï¼Œè‡´åŠ›äºä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆã€‚
        
        æ¬¢è¿è®¿é—®[17AIæŠ€æœ¯ç¤¾åŒº](http://17aitech.com)è·å–æ›´å¤šAIç›¸å…³èµ„æºå’Œæ•™ç¨‹ï¼
        """)
        
        st.markdown("---")
    
    # ä¾§è¾¹æ å¯¼èˆª
    st.sidebar.header("ğŸ¯ åŠŸèƒ½å¯¼èˆª")
    menu = st.sidebar.selectbox(
        "è¯·é€‰æ‹©åŠŸèƒ½",
        ["ğŸ“Š æ•°æ®åˆ†æ", "ğŸ§¹ æ•°æ®æ¸…æ´—", "ğŸ”„ æ¨¡å‹è®­ç»ƒ", "âœ¨ æ•ˆæœéªŒè¯"]
    )
    
    if menu == "ğŸ“Š æ•°æ®åˆ†æ":
        data_analysis()
    elif menu == "ğŸ§¹ æ•°æ®æ¸…æ´—":
        data_cleaning()
    elif menu == "ğŸ”„ æ¨¡å‹è®­ç»ƒ":
        model_training()
    elif menu == "âœ¨ æ•ˆæœéªŒè¯":
        model_validation()

if __name__ == "__main__":
    main() 